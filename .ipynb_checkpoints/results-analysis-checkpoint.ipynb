{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-Quantum RPKI Validation Results - Comprehensive JSON Analysis\n",
        "\n",
        "**Scientific Analysis of Post-Quantum Signature Algorithms in RPKI**\n",
        "\n",
        "This notebook provides comprehensive analysis of post-quantum RPKI measurements from the JSON output of `validate.py`, including:\n",
        "\n",
        "- **Repository Metrics**: File counts, sizes, type breakdowns (certificates, ROAs, manifests, CRLs)\n",
        "- **Signature Verification**: Verification rates, ASN.1 extraction success, timing statistics\n",
        "- **Performance Analysis**: Validation times, throughput, per-signature verification times\n",
        "- **Size Analysis**: Signature sizes, public key sizes, overhead calculations\n",
        "- **Relative Performance**: Comparisons vs ECDSA baseline\n",
        "- **Statistical Analysis**: Min/max/avg/median metrics for all measurements\n",
        "\n",
        "**Data Source**: JSON output from `validate.py` (comprehensive nested structure)  \n",
        "**Author**: Post-Quantum RPKI Research Team  \n",
        "**Date**: December 2025  \n",
        "**Dataset**: Real-world RPKI repository measurements (118,068+ objects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Publication-quality plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-paper')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-paper')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'font.size': 11,\n",
        "    'font.family': 'serif',\n",
        "    'axes.labelsize': 12,\n",
        "    'axes.titlesize': 14,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'legend.fontsize': 10,\n",
        "    'figure.titlesize': 16,\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'savefig.bbox': 'tight',\n",
        "    'savefig.pad_inches': 0.1\n",
        "})\n",
        "\n",
        "print(\"✓ Libraries loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data - Primary focus on JSON for detailed nested metrics\n",
        "# Try results/ directory first, then fallback to root\n",
        "json_path = Path(\"results/results.json\")\n",
        "csv_path = Path(\"results/results.csv\")\n",
        "\n",
        "# Fallback to root if results/ doesn't exist\n",
        "if not json_path.exists():\n",
        "    json_path = Path(\"results.json\")\n",
        "if not csv_path.exists():\n",
        "    csv_path = Path(\"results.csv\")\n",
        "\n",
        "# Fallback to Docker paths\n",
        "if not json_path.exists():\n",
        "    json_path = Path(\"/work/results/results.json\")\n",
        "if not csv_path.exists():\n",
        "    csv_path = Path(\"/work/results/results.csv\")\n",
        "\n",
        "# Final fallback\n",
        "if not json_path.exists():\n",
        "    json_path = Path(\"/work/results.json\")\n",
        "if not csv_path.exists():\n",
        "    csv_path = Path(\"/work/results.csv\")\n",
        "\n",
        "if not json_path.exists():\n",
        "    raise FileNotFoundError(\"results.json not found. Please run validate.py first to generate JSON data.\")\n",
        "\n",
        "# Load JSON data (primary source - contains all nested metrics)\n",
        "with open(json_path, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "metadata = json_data.get('experiment_metadata', {})\n",
        "results_list = json_data.get('results', [])\n",
        "validation_errors = json_data.get('validation_errors', [])\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "df = pd.DataFrame(results_list)\n",
        "\n",
        "# Also load CSV for comparison (flattened structure)\n",
        "df_csv = None\n",
        "if csv_path.exists():\n",
        "    df_csv = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"✓ Loaded JSON data: {len(results_list)} algorithm results\")\n",
        "print(f\"✓ Experiment date: {metadata.get('date', 'Unknown')}\")\n",
        "print(f\"✓ Total objects: {metadata.get('total_objects', 'Unknown'):,}\")\n",
        "print(f\"✓ ASN.1 extraction available: {metadata.get('asn1_extraction_available', False)}\")\n",
        "print(f\"✓ OQS library available: {metadata.get('oqs_available', False)}\")\n",
        "print(f\"\\nAvailable metrics in JSON:\")\n",
        "print(f\"  - File statistics: file_count, total_size_gb, file_type_breakdown\")\n",
        "print(f\"  - Signature verification: verified, failed, verification times, signature sizes\")\n",
        "print(f\"  - Performance: validation_time_sec, objects_per_second, scan_time_sec\")\n",
        "print(f\"  - Object types: certificates, ROAs, manifests, CRLs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Experiment Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment overview\n",
        "print(\"=\" * 80)\n",
        "print(\"  FIRST REAL POST-QUANTUM RPKI MEASUREMENTS (December 2025)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nExperiment Date: {metadata.get('date', 'Unknown')}\")\n",
        "print(f\"Total Objects Validated: {metadata.get('total_objects', 'Unknown'):,}\")\n",
        "print(f\"Total Algorithms Tested: {len(df)}\")\n",
        "print(f\"Successful Validations: {df['validation_success'].sum()}/{len(df)}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary table with key metrics\n",
        "summary_cols = ['algorithm', 'algorithm_standardized', 'nist_security_level', \n",
        "                'file_count', 'total_size_gb', 'validation_time_min', 'validation_success']\n",
        "\n",
        "# Extract file type breakdown if available\n",
        "if 'file_type_breakdown' in df.columns:\n",
        "    # Create a summary of file types\n",
        "    file_types_summary = []\n",
        "    for idx, row in df.iterrows():\n",
        "        ftb = row.get('file_type_breakdown', {})\n",
        "        if isinstance(ftb, dict):\n",
        "            types_str = ', '.join([f\"{k}:{v}\" for k, v in ftb.items() if v > 0])\n",
        "            file_types_summary.append(types_str)\n",
        "        else:\n",
        "            file_types_summary.append('N/A')\n",
        "    df['file_types'] = file_types_summary\n",
        "\n",
        "summary_df = df[summary_cols].copy()\n",
        "summary_df['validation_success'] = summary_df['validation_success'].map({True: 'PASS', False: 'FAIL'})\n",
        "summary_df.columns = ['Algorithm', 'Standardized Name', 'NIST Level', \n",
        "                     'File Count', 'Size (GB)', 'Time (min)', 'Status']\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Relative Performance vs ECDSA Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate relative metrics vs baseline\n",
        "baseline = None\n",
        "baseline_idx = None\n",
        "if 'ecdsa-baseline' in df['algorithm'].values:\n",
        "    baseline_idx = df[df['algorithm'] == 'ecdsa-baseline'].index[0]\n",
        "    baseline = df.iloc[baseline_idx]\n",
        "\n",
        "if baseline is not None:\n",
        "    df['size_overhead'] = ((df['total_size_gb'] / baseline['total_size_gb'] - 1) * 100).round(2)\n",
        "    df['time_overhead'] = ((df['validation_time_sec'] / baseline['validation_time_sec'] - 1) * 100).round(2)\n",
        "    \n",
        "    comparison_df = df[df['algorithm'] != 'ecdsa-baseline'][['algorithm', 'size_overhead', 'time_overhead']].copy()\n",
        "    comparison_df.columns = ['Algorithm', 'Size Overhead (%)', 'Time Overhead (%)']\n",
        "    comparison_df['Size Overhead (%)'] = comparison_df['Size Overhead (%)'].apply(lambda x: f\"{x:+.1f}%\")\n",
        "    comparison_df['Time Overhead (%)'] = comparison_df['Time Overhead (%)'].apply(lambda x: f\"{x:+.1f}%\")\n",
        "    comparison_df\n",
        "else:\n",
        "    print(\"Baseline data not available for comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n",
        "\n",
        "### 5.1 Validation Time Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = ['#2ecc71' if x else '#e74c3c' for x in df['validation_success']]\n",
        "bars = ax.bar(df['algorithm'], df['validation_time_min'], color=colors, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "ax.set_xlabel('Algorithm', fontweight='bold')\n",
        "ax.set_ylabel('Validation Time (minutes)', fontweight='bold')\n",
        "ax.set_title('RPKI Validation Time: Post-Quantum vs Classical', fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Repository Size Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = ['#3498db' if x else '#e74c3c' for x in df['validation_success']]\n",
        "bars = ax.bar(df['algorithm'], df['total_size_gb'], color=colors, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "ax.set_xlabel('Algorithm', fontweight='bold')\n",
        "ax.set_ylabel('Repository Size (GB)', fontweight='bold')\n",
        "ax.set_title('RPKI Repository Size: Post-Quantum vs Classical', fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Relative Performance vs Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if baseline is not None:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    pq_df = df[df['algorithm'] != 'ecdsa-baseline']\n",
        "    \n",
        "    # Size overhead\n",
        "    colors_size = ['#e67e22' if x >= 0 else '#27ae60' for x in pq_df['size_overhead']]\n",
        "    bars1 = ax1.bar(pq_df['algorithm'], pq_df['size_overhead'], color=colors_size, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "    ax1.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "    ax1.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax1.set_ylabel('Size Overhead (%)', fontweight='bold')\n",
        "    ax1.set_title('Repository Size Overhead vs ECDSA Baseline', fontweight='bold')\n",
        "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax1.set_axisbelow(True)\n",
        "    \n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height, f'{height:+.1f}%',\n",
        "                ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')\n",
        "    \n",
        "    # Time overhead\n",
        "    colors_time = ['#e67e22' if x >= 0 else '#27ae60' for x in pq_df['time_overhead']]\n",
        "    bars2 = ax2.bar(pq_df['algorithm'], pq_df['time_overhead'], color=colors_time, alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "    ax2.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax2.set_ylabel('Time Overhead (%)', fontweight='bold')\n",
        "    ax2.set_title('Validation Time Overhead vs ECDSA Baseline', fontweight='bold')\n",
        "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax2.set_axisbelow(True)\n",
        "    \n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height, f'{height:+.1f}%',\n",
        "                ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')\n",
        "    \n",
        "    plt.setp([ax1.xaxis.get_majorticklabels(), ax2.xaxis.get_majorticklabels()], rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Baseline data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Daily Delta Analysis (Bandwidth Overhead)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate daily bandwidth overhead (2% daily update rate typical for RPKI)\n",
        "daily_update_rate = 0.02\n",
        "\n",
        "if baseline is not None:\n",
        "    baseline_size = baseline['total_size_bytes']\n",
        "    baseline_daily_mb = (baseline_size * daily_update_rate) / (1024**2)\n",
        "    \n",
        "    print(f\"Baseline repository size: {baseline_size / (1024**3):.2f} GB\")\n",
        "    print(f\"Assumed daily update rate: {daily_update_rate*100:.1f}%\")\n",
        "    print(f\"Baseline daily delta: {baseline_daily_mb:.2f} MB/day\\n\")\n",
        "    \n",
        "    delta_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        if row['algorithm'] != 'ecdsa-baseline' and row['total_size_bytes'] > 0:\n",
        "            size_bytes = row['total_size_bytes']\n",
        "            overhead_bytes = size_bytes - baseline_size\n",
        "            overhead_percent = (overhead_bytes / baseline_size * 100) if baseline_size > 0 else 0\n",
        "            daily_delta_mb = ((size_bytes * daily_update_rate) - (baseline_size * daily_update_rate)) / (1024**2)\n",
        "            \n",
        "            delta_data.append({\n",
        "                'Algorithm': row['algorithm'],\n",
        "                'Size Overhead (%)': f\"{overhead_percent:+.1f}%\",\n",
        "                'Daily Delta (MB/day)': f\"{daily_delta_mb:+.2f}\"\n",
        "            })\n",
        "    \n",
        "    if delta_data:\n",
        "        delta_df = pd.DataFrame(delta_data)\n",
        "        delta_df\n",
        "else:\n",
        "    print(\"Baseline data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if baseline is not None:\n",
        "    for _, row in df.iterrows():\n",
        "        if row['algorithm'] != 'ecdsa-baseline':\n",
        "            print(f\"\\n{row['algorithm'].upper()}:\")\n",
        "            print(f\"  • Size overhead: {row['size_overhead']:+.1f}% vs ECDSA\")\n",
        "            print(f\"  • Time overhead: {row['time_overhead']:+.1f}% vs ECDSA\")\n",
        "            if row['validation_success']:\n",
        "                status_msg = row.get('validation_status', 'PASS')\n",
        "                if 'rpki-client' in str(status_msg).lower():\n",
        "                    print(f\"  • Status: ✓ PASS with real rpki-client validation\")\n",
        "                else:\n",
        "                    print(f\"  • Status: ✓ PASS\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SCIENTIFIC CONTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nThis dataset represents the first real-world measurements of\")\n",
        "print(\"NIST post-quantum signature algorithms (ML-DSA, Falcon) applied\")\n",
        "print(\"to the global RPKI repository at scale.\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Detailed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. Detailed Signature Verification Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract signature verification metrics from nested JSON structure\n",
        "# Includes all new comprehensive metrics we've added\n",
        "sig_verification_data = []\n",
        "\n",
        "for result in results_list:\n",
        "    algo = result.get('algorithm', 'unknown')\n",
        "    sig_ver = result.get('signature_verification', {})\n",
        "    \n",
        "    if sig_ver and isinstance(sig_ver, dict):\n",
        "        # Extract all available metrics\n",
        "        sig_verification_data.append({\n",
        "            'algorithm': algo,\n",
        "            'sampled': sig_ver.get('sampled', 0),\n",
        "            'verified': sig_ver.get('verified', 0),\n",
        "            'failed': sig_ver.get('failed', 0),\n",
        "            'asn1_failures': sig_ver.get('asn1_extraction_failures', 0),\n",
        "            'verify_time_sec': sig_ver.get('verify_time_sec', 0),\n",
        "            'time_per_file_ms': sig_ver.get('time_per_file_sec', 0) * 1000,\n",
        "            'estimated_total_time_sec': sig_ver.get('estimated_total_time_sec', 0),\n",
        "            'verification_rate_per_sec': sig_ver.get('verification_rate_per_sec', 0),\n",
        "            'avg_verify_time_ms': sig_ver.get('avg_verify_time_ms', 0),\n",
        "            'min_verify_time_ms': sig_ver.get('min_verify_time_ms', 0),\n",
        "            'max_verify_time_ms': sig_ver.get('max_verify_time_ms', 0),\n",
        "            'median_verify_time_ms': sig_ver.get('median_verify_time_ms', 0),\n",
        "            'p25_verify_time_ms': sig_ver.get('p25_verify_time_ms', 0),\n",
        "            'p50_verify_time_ms': sig_ver.get('p50_verify_time_ms', 0),\n",
        "            'p75_verify_time_ms': sig_ver.get('p75_verify_time_ms', 0),\n",
        "            'p95_verify_time_ms': sig_ver.get('p95_verify_time_ms', 0),\n",
        "            'p99_verify_time_ms': sig_ver.get('p99_verify_time_ms', 0),\n",
        "            'signature_size_avg_bytes': sig_ver.get('signature_size_avg_bytes', 0),\n",
        "            'signature_size_min_bytes': sig_ver.get('signature_size_min_bytes', 0),\n",
        "            'signature_size_max_bytes': sig_ver.get('signature_size_max_bytes', 0),\n",
        "            'signature_size_p25_bytes': sig_ver.get('signature_size_p25_bytes', 0),\n",
        "            'signature_size_p50_bytes': sig_ver.get('signature_size_p50_bytes', 0),\n",
        "            'signature_size_p75_bytes': sig_ver.get('signature_size_p75_bytes', 0),\n",
        "            'signature_size_p95_bytes': sig_ver.get('signature_size_p95_bytes', 0),\n",
        "            'signature_size_p99_bytes': sig_ver.get('signature_size_p99_bytes', 0),\n",
        "            'expected_signature_size_bytes': sig_ver.get('expected_signature_size_bytes', 0),\n",
        "            'signature_size_variance': sig_ver.get('signature_size_variance', 0),\n",
        "            'public_key_size_avg_bytes': sig_ver.get('public_key_size_avg_bytes', 0),\n",
        "            'public_key_size_min_bytes': sig_ver.get('public_key_size_min_bytes', 0),\n",
        "            'public_key_size_max_bytes': sig_ver.get('public_key_size_max_bytes', 0),\n",
        "            'public_key_size_p50_bytes': sig_ver.get('public_key_size_p50_bytes', 0),\n",
        "            'expected_public_key_size_bytes': sig_ver.get('expected_public_key_size_bytes', 0),\n",
        "            'public_key_size_variance': sig_ver.get('public_key_size_variance', 0),\n",
        "            'verification_success_rate': (sig_ver.get('verified', 0) / sig_ver.get('sampled', 1)) * 100 if sig_ver.get('sampled', 0) > 0 else 0\n",
        "        })\n",
        "    else:\n",
        "        # No signature verification data\n",
        "        sig_verification_data.append({\n",
        "            'algorithm': algo,\n",
        "            'sampled': 0,\n",
        "            'verified': 0,\n",
        "            'failed': 0,\n",
        "            'asn1_failures': 0,\n",
        "            'verify_time_sec': 0,\n",
        "            'time_per_file_ms': 0,\n",
        "            'estimated_total_time_sec': 0,\n",
        "            'verification_rate_per_sec': 0,\n",
        "            'avg_verify_time_ms': 0,\n",
        "            'min_verify_time_ms': 0,\n",
        "            'max_verify_time_ms': 0,\n",
        "            'median_verify_time_ms': 0,\n",
        "            'p25_verify_time_ms': 0,\n",
        "            'p50_verify_time_ms': 0,\n",
        "            'p75_verify_time_ms': 0,\n",
        "            'p95_verify_time_ms': 0,\n",
        "            'p99_verify_time_ms': 0,\n",
        "            'signature_size_avg_bytes': 0,\n",
        "            'signature_size_min_bytes': 0,\n",
        "            'signature_size_max_bytes': 0,\n",
        "            'signature_size_p25_bytes': 0,\n",
        "            'signature_size_p50_bytes': 0,\n",
        "            'signature_size_p75_bytes': 0,\n",
        "            'signature_size_p95_bytes': 0,\n",
        "            'signature_size_p99_bytes': 0,\n",
        "            'expected_signature_size_bytes': 0,\n",
        "            'signature_size_variance': 0,\n",
        "            'public_key_size_avg_bytes': 0,\n",
        "            'public_key_size_min_bytes': 0,\n",
        "            'public_key_size_max_bytes': 0,\n",
        "            'public_key_size_p50_bytes': 0,\n",
        "            'expected_public_key_size_bytes': 0,\n",
        "            'public_key_size_variance': 0,\n",
        "            'verification_success_rate': 0\n",
        "        })\n",
        "\n",
        "sig_df = pd.DataFrame(sig_verification_data)\n",
        "sig_df = sig_df[sig_df['sampled'] > 0]  # Only show algorithms with signature verification data\n",
        "\n",
        "if len(sig_df) > 0:\n",
        "    print(\"Signature Verification Metrics (Comprehensive):\")\n",
        "    display_cols = ['algorithm', 'sampled', 'verified', 'failed', 'asn1_failures', \n",
        "                    'verification_success_rate', 'avg_verify_time_ms', 'verification_rate_per_sec',\n",
        "                    'signature_size_avg_bytes', 'expected_signature_size_bytes',\n",
        "                    'public_key_size_avg_bytes', 'expected_public_key_size_bytes']\n",
        "    display_df = sig_df[display_cols].copy()\n",
        "    display_df.columns = ['Algorithm', 'Sampled', 'Verified', 'Failed', 'ASN.1 Errors', \n",
        "                          'Success Rate (%)', 'Avg Verify Time (ms)', 'Rate (sig/s)',\n",
        "                          'Sig Size (bytes)', 'Expected Sig Size (bytes)',\n",
        "                          'PubKey Size (bytes)', 'Expected PubKey Size (bytes)']\n",
        "    display_df['Success Rate (%)'] = display_df['Success Rate (%)'].round(1)\n",
        "    display_df['Avg Verify Time (ms)'] = display_df['Avg Verify Time (ms)'].round(2)\n",
        "    display_df['Rate (sig/s)'] = display_df['Rate (sig/s)'].round(1)\n",
        "    display_df['Sig Size (bytes)'] = display_df['Sig Size (bytes)'].round(0)\n",
        "    display_df['PubKey Size (bytes)'] = display_df['PubKey Size (bytes)'].round(0)\n",
        "    display(display_df)\n",
        "    \n",
        "    # Show detailed metrics summary\n",
        "    print(\"\\nDetailed Metrics Summary:\")\n",
        "    detailed_cols = ['algorithm', 'p25_verify_time_ms', 'p50_verify_time_ms', 'p75_verify_time_ms', \n",
        "                     'p95_verify_time_ms', 'p99_verify_time_ms', 'signature_size_variance', \n",
        "                     'public_key_size_variance']\n",
        "    detailed_df = sig_df[detailed_cols].copy()\n",
        "    detailed_df.columns = ['Algorithm', 'P25 (ms)', 'P50 (ms)', 'P75 (ms)', 'P95 (ms)', 'P99 (ms)',\n",
        "                           'Sig Size Variance', 'PubKey Size Variance']\n",
        "    display(detailed_df)\n",
        "else:\n",
        "    print(\"No signature verification data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Signature Verification Timing Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize signature verification timing statistics with all new metrics\n",
        "if len(sig_df) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    \n",
        "    # Average verification time\n",
        "    ax1 = axes[0, 0]\n",
        "    bars1 = ax1.bar(sig_df['algorithm'], sig_df['avg_verify_time_ms'], \n",
        "                    color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "    ax1.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax1.set_ylabel('Average Verification Time (ms)', fontweight='bold')\n",
        "    ax1.set_title('Average Signature Verification Time', fontweight='bold')\n",
        "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax1.set_axisbelow(True)\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Verification rate\n",
        "    ax2 = axes[0, 1]\n",
        "    bars2 = ax2.bar(sig_df['algorithm'], sig_df['verification_rate_per_sec'],\n",
        "                    color='#2ecc71', alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "    ax2.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax2.set_ylabel('Verification Rate (signatures/sec)', fontweight='bold')\n",
        "    ax2.set_title('Signature Verification Throughput', fontweight='bold')\n",
        "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax2.set_axisbelow(True)\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Verification success rate\n",
        "    ax3 = axes[0, 2]\n",
        "    bars3 = ax3.bar(sig_df['algorithm'], sig_df['verification_success_rate'],\n",
        "                    color='#e67e22', alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "    ax3.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax3.set_ylabel('Success Rate (%)', fontweight='bold')\n",
        "    ax3.set_title('Signature Verification Success Rate', fontweight='bold')\n",
        "    ax3.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax3.set_axisbelow(True)\n",
        "    for bar in bars3:\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Percentile distribution of verification times\n",
        "    ax4 = axes[1, 0]\n",
        "    x = np.arange(len(sig_df))\n",
        "    width = 0.15\n",
        "    ax4.bar(x - 2*width, sig_df['p25_verify_time_ms'], width, label='P25', color='#27ae60', alpha=0.7, edgecolor='black')\n",
        "    ax4.bar(x - width, sig_df['p50_verify_time_ms'], width, label='P50 (Median)', color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    ax4.bar(x, sig_df['p75_verify_time_ms'], width, label='P75', color='#9b59b6', alpha=0.7, edgecolor='black')\n",
        "    ax4.bar(x + width, sig_df['p95_verify_time_ms'], width, label='P95', color='#e67e22', alpha=0.7, edgecolor='black')\n",
        "    ax4.bar(x + 2*width, sig_df['p99_verify_time_ms'], width, label='P99', color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "    ax4.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax4.set_ylabel('Verification Time (ms)', fontweight='bold')\n",
        "    ax4.set_title('Verification Time Percentiles', fontweight='bold')\n",
        "    ax4.set_xticks(x)\n",
        "    ax4.set_xticklabels(sig_df['algorithm'], rotation=45, ha='right')\n",
        "    ax4.legend(fontsize=8)\n",
        "    ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax4.set_axisbelow(True)\n",
        "    \n",
        "    # Signature sizes: Actual vs Expected\n",
        "    ax5 = axes[1, 1]\n",
        "    x = np.arange(len(sig_df))\n",
        "    width = 0.35\n",
        "    ax5.bar(x - width/2, sig_df['signature_size_avg_bytes'], width, \n",
        "           label='Actual Avg', color='#9b59b6', alpha=0.7, edgecolor='black')\n",
        "    ax5.bar(x + width/2, sig_df['expected_signature_size_bytes'], width,\n",
        "           label='Expected', color='#34495e', alpha=0.7, edgecolor='black')\n",
        "    ax5.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax5.set_ylabel('Signature Size (bytes)', fontweight='bold')\n",
        "    ax5.set_title('Signature Size: Actual vs Expected', fontweight='bold')\n",
        "    ax5.set_xticks(x)\n",
        "    ax5.set_xticklabels(sig_df['algorithm'], rotation=45, ha='right')\n",
        "    ax5.legend()\n",
        "    ax5.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax5.set_axisbelow(True)\n",
        "    \n",
        "    # Public key sizes: Actual vs Expected\n",
        "    ax6 = axes[1, 2]\n",
        "    x = np.arange(len(sig_df))\n",
        "    width = 0.35\n",
        "    ax6.bar(x - width/2, sig_df['public_key_size_avg_bytes'], width, \n",
        "           label='Actual Avg', color='#16a085', alpha=0.7, edgecolor='black')\n",
        "    ax6.bar(x + width/2, sig_df['expected_public_key_size_bytes'], width,\n",
        "           label='Expected', color='#1abc9c', alpha=0.7, edgecolor='black')\n",
        "    ax6.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax6.set_ylabel('Public Key Size (bytes)', fontweight='bold')\n",
        "    ax6.set_title('Public Key Size: Actual vs Expected', fontweight='bold')\n",
        "    ax6.set_xticks(x)\n",
        "    ax6.set_xticklabels(sig_df['algorithm'], rotation=45, ha='right')\n",
        "    ax6.legend()\n",
        "    ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax6.set_axisbelow(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No signature verification data available for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. File Type Breakdown Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and analyze file type breakdowns\n",
        "file_type_data = []\n",
        "\n",
        "for result in results_list:\n",
        "    algo = result.get('algorithm', 'unknown')\n",
        "    ftb = result.get('file_type_breakdown', {})\n",
        "    \n",
        "    if isinstance(ftb, dict):\n",
        "        file_type_data.append({\n",
        "            'algorithm': algo,\n",
        "            'certificates': ftb.get('certificates', 0),\n",
        "            'roas': ftb.get('roas', 0),\n",
        "            'manifests': ftb.get('manifests', 0),\n",
        "            'crls': ftb.get('crls', 0),\n",
        "            'other': ftb.get('other', 0),\n",
        "            'total': result.get('file_count', 0)\n",
        "        })\n",
        "    else:\n",
        "        file_type_data.append({\n",
        "            'algorithm': algo,\n",
        "            'certificates': 0,\n",
        "            'roas': 0,\n",
        "            'manifests': 0,\n",
        "            'crls': 0,\n",
        "            'other': 0,\n",
        "            'total': result.get('file_count', 0)\n",
        "        })\n",
        "\n",
        "ftb_df = pd.DataFrame(file_type_data)\n",
        "\n",
        "if len(ftb_df) > 0:\n",
        "    print(\"File Type Breakdown by Algorithm:\")\n",
        "    display(ftb_df)\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(len(ftb_df))\n",
        "    width = 0.15\n",
        "    \n",
        "    ax.bar(x - 2*width, ftb_df['certificates'], width, label='Certificates', color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x - width, ftb_df['roas'], width, label='ROAs', color='#2ecc71', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x, ftb_df['manifests'], width, label='Manifests', color='#e67e22', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x + width, ftb_df['crls'], width, label='CRLs', color='#9b59b6', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x + 2*width, ftb_df['other'], width, label='Other', color='#95a5a6', alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    ax.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax.set_ylabel('File Count', fontweight='bold')\n",
        "    ax.set_title('File Type Distribution by Algorithm', fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(ftb_df['algorithm'], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.set_axisbelow(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No file type breakdown data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Object Type Breakdown from Signature Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract object type breakdown and per-type metrics from signature verification\n",
        "object_type_data = []\n",
        "per_type_metrics_data = []\n",
        "\n",
        "for result in results_list:\n",
        "    algo = result.get('algorithm', 'unknown')\n",
        "    sig_ver = result.get('signature_verification', {})\n",
        "    \n",
        "    if sig_ver and isinstance(sig_ver, dict):\n",
        "        # Object type breakdown\n",
        "        otb = sig_ver.get('object_type_breakdown', {})\n",
        "        if isinstance(otb, dict):\n",
        "            object_type_data.append({\n",
        "                'algorithm': algo,\n",
        "                'certificate': otb.get('certificate', 0),\n",
        "                'roa': otb.get('roa', 0),\n",
        "                'manifest': otb.get('manifest', 0),\n",
        "                'crl': otb.get('crl', 0),\n",
        "                'total_sampled': sig_ver.get('sampled', 0)\n",
        "            })\n",
        "        else:\n",
        "            object_type_data.append({\n",
        "                'algorithm': algo,\n",
        "                'certificate': 0,\n",
        "                'roa': 0,\n",
        "                'manifest': 0,\n",
        "                'crl': 0,\n",
        "                'total_sampled': sig_ver.get('sampled', 0)\n",
        "            })\n",
        "        \n",
        "        # Per-type metrics (detailed breakdown)\n",
        "        ptm = sig_ver.get('per_type_metrics', {})\n",
        "        if isinstance(ptm, dict):\n",
        "            for obj_type, metrics in ptm.items():\n",
        "                if isinstance(metrics, dict) and metrics.get('count', 0) > 0:\n",
        "                    per_type_metrics_data.append({\n",
        "                        'algorithm': algo,\n",
        "                        'object_type': obj_type,\n",
        "                        'count': metrics.get('count', 0),\n",
        "                        'verified': metrics.get('verified', 0),\n",
        "                        'failed': metrics.get('failed', 0),\n",
        "                        'verification_rate': metrics.get('verification_rate', 0),\n",
        "                        'avg_verify_time_ms': metrics.get('avg_verify_time_ms', 0),\n",
        "                        'avg_sig_size_bytes': metrics.get('avg_sig_size_bytes', 0),\n",
        "                        'avg_pubkey_size_bytes': metrics.get('avg_pubkey_size_bytes', 0),\n",
        "                        'ee_certs_found': metrics.get('ee_certs_found', 0),\n",
        "                        'issuer_certs_found': metrics.get('issuer_certs_found', 0),\n",
        "                        'cms_valid_count': metrics.get('cms_valid_count', 0),\n",
        "                        'ee_cert_valid_count': metrics.get('ee_cert_valid_count', 0),\n",
        "                        'both_valid_count': metrics.get('both_valid_count', 0)\n",
        "                    })\n",
        "    else:\n",
        "        object_type_data.append({\n",
        "            'algorithm': algo,\n",
        "            'certificate': 0,\n",
        "            'roa': 0,\n",
        "            'manifest': 0,\n",
        "            'crl': 0,\n",
        "            'total_sampled': 0\n",
        "        })\n",
        "\n",
        "otb_df = pd.DataFrame(object_type_data)\n",
        "otb_df = otb_df[otb_df['total_sampled'] > 0]  # Only show algorithms with data\n",
        "\n",
        "if len(otb_df) > 0:\n",
        "    print(\"Object Type Breakdown from Signature Verification Sample:\")\n",
        "    display(otb_df)\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(len(otb_df))\n",
        "    width = 0.2\n",
        "    \n",
        "    ax.bar(x - 1.5*width, otb_df['certificate'], width, label='Certificate', color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x - 0.5*width, otb_df['roa'], width, label='ROA', color='#2ecc71', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x + 0.5*width, otb_df['manifest'], width, label='Manifest', color='#e67e22', alpha=0.7, edgecolor='black')\n",
        "    ax.bar(x + 1.5*width, otb_df['crl'], width, label='CRL', color='#9b59b6', alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    ax.set_xlabel('Algorithm', fontweight='bold')\n",
        "    ax.set_ylabel('Object Count (from verification sample)', fontweight='bold')\n",
        "    ax.set_title('Object Type Distribution in Verification Sample', fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(otb_df['algorithm'], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.set_axisbelow(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No object type breakdown data available from signature verification\")\n",
        "\n",
        "# Per-type detailed metrics\n",
        "if len(per_type_metrics_data) > 0:\n",
        "    ptm_df = pd.DataFrame(per_type_metrics_data)\n",
        "    print(\"\\nPer-Type Detailed Metrics:\")\n",
        "    display_cols = ['algorithm', 'object_type', 'count', 'verified', 'failed', 'verification_rate',\n",
        "                    'avg_verify_time_ms', 'avg_sig_size_bytes', 'avg_pubkey_size_bytes',\n",
        "                    'ee_certs_found', 'issuer_certs_found', 'cms_valid_count', 'ee_cert_valid_count', 'both_valid_count']\n",
        "    display_ptm = ptm_df[display_cols].copy()\n",
        "    display_ptm.columns = ['Algorithm', 'Type', 'Count', 'Verified', 'Failed', 'Rate (%)',\n",
        "                           'Avg Time (ms)', 'Avg Sig (bytes)', 'Avg PubKey (bytes)',\n",
        "                           'EE Certs', 'Issuer Certs', 'CMS Valid', 'EE Valid', 'Both Valid']\n",
        "    display(display_ptm)\n",
        "else:\n",
        "    print(\"\\nNo per-type metrics data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Complete JSON Data Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display full JSON structure for one algorithm as example\n",
        "print(\"Example JSON structure for one algorithm:\")\n",
        "print(\"=\" * 80)\n",
        "if len(results_list) > 0:\n",
        "    import json\n",
        "    example = results_list[0]\n",
        "    print(json.dumps(example, indent=2))\n",
        "else:\n",
        "    print(\"No results available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Statistical Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Comprehensive Metrics Tables\n",
        "\n",
        "### 13.1 Complete Metrics Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive metrics table with all available data\n",
        "comprehensive_data = []\n",
        "\n",
        "for result in results_list:\n",
        "    algo = result.get('algorithm', 'unknown')\n",
        "    sig_ver = result.get('signature_verification', {})\n",
        "    \n",
        "    row = {\n",
        "        'Algorithm': algo,\n",
        "        'Standardized': result.get('algorithm_standardized', ''),\n",
        "        'NIST Level': result.get('nist_security_level', ''),\n",
        "        'File Count': result.get('file_count', 0),\n",
        "        'Total Size (GB)': result.get('total_size_gb', 0),\n",
        "        'Validation Time (min)': result.get('validation_time_min', 0),\n",
        "        'Objects/sec': result.get('objects_per_second', 0),\n",
        "    }\n",
        "    \n",
        "    # Add signature verification metrics if available\n",
        "    if sig_ver and isinstance(sig_ver, dict) and sig_ver.get('sampled', 0) > 0:\n",
        "        row.update({\n",
        "            'Sig Verified': sig_ver.get('verified', 0),\n",
        "            'Sig Failed': sig_ver.get('failed', 0),\n",
        "            'Sig Success Rate (%)': round((sig_ver.get('verified', 0) / sig_ver.get('sampled', 1)) * 100, 1),\n",
        "            'Avg Verify Time (ms)': round(sig_ver.get('avg_verify_time_ms', 0), 2),\n",
        "            'Verify Rate (sig/s)': round(sig_ver.get('verification_rate_per_sec', 0), 1),\n",
        "            'Sig Size (bytes)': round(sig_ver.get('signature_size_avg_bytes', 0), 0),\n",
        "            'PubKey Size (bytes)': round(sig_ver.get('public_key_size_avg_bytes', 0), 0),\n",
        "            'Expected Sig Size': sig_ver.get('expected_signature_size_bytes', 0),\n",
        "            'Expected PubKey Size': sig_ver.get('expected_public_key_size_bytes', 0),\n",
        "        })\n",
        "    else:\n",
        "        row.update({\n",
        "            'Sig Verified': 0,\n",
        "            'Sig Failed': 0,\n",
        "            'Sig Success Rate (%)': 0,\n",
        "            'Avg Verify Time (ms)': 0,\n",
        "            'Verify Rate (sig/s)': 0,\n",
        "            'Sig Size (bytes)': 0,\n",
        "            'PubKey Size (bytes)': 0,\n",
        "            'Expected Sig Size': 0,\n",
        "            'Expected PubKey Size': 0,\n",
        "        })\n",
        "    \n",
        "    # Add relative metrics if baseline available\n",
        "    if baseline is not None and algo != 'ecdsa-baseline':\n",
        "        size_oh = ((result['total_size_gb'] / baseline['total_size_gb'] - 1) * 100)\n",
        "        time_oh = ((result['validation_time_sec'] / baseline['validation_time_sec'] - 1) * 100)\n",
        "        row['Size Overhead (%)'] = round(size_oh, 1)\n",
        "        row['Time Overhead (%)'] = round(time_oh, 1)\n",
        "    else:\n",
        "        row['Size Overhead (%)'] = 0\n",
        "        row['Time Overhead (%)'] = 0\n",
        "    \n",
        "    comprehensive_data.append(row)\n",
        "\n",
        "comp_df = pd.DataFrame(comprehensive_data)\n",
        "display(comp_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 13.2 Export to LaTeX Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export comprehensive table to LaTeX format for papers\n",
        "latex_table = comp_df.to_latex(index=False, float_format=\"%.2f\", escape=False)\n",
        "print(\"LaTeX Table (for papers):\")\n",
        "print(\"=\" * 80)\n",
        "print(latex_table)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to file\n",
        "latex_path = Path(\"results/results_table.tex\")\n",
        "with open(latex_path, 'w') as f:\n",
        "    f.write(latex_table)\n",
        "print(f\"\\nLaTeX table saved to: {latex_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 13.3 Export to CSV for Further Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export comprehensive table to CSV\n",
        "comp_csv_path = Path(\"results/comprehensive_metrics.csv\")\n",
        "comp_df.to_csv(comp_csv_path, index=False)\n",
        "print(f\"Comprehensive metrics table saved to: {comp_csv_path}\")\n",
        "print(f\"Rows: {len(comp_df)}, Columns: {len(comp_df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive statistical summary with all new metrics\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE STATISTICAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if baseline is not None:\n",
        "    print(f\"\\nBaseline (ECDSA):\")\n",
        "    print(f\"  File count: {baseline['file_count']:,}\")\n",
        "    print(f\"  Total size: {baseline['total_size_gb']:.3f} GB\")\n",
        "    print(f\"  Validation time: {baseline['validation_time_sec']:.2f} seconds ({baseline['validation_time_min']:.2f} minutes)\")\n",
        "    print(f\"  Objects per second: {baseline.get('objects_per_second', 0):.2f}\")\n",
        "\n",
        "print(f\"\\nPost-Quantum Algorithms:\")\n",
        "for result in results_list:\n",
        "    if result['algorithm'] != 'ecdsa-baseline':\n",
        "        algo = result['algorithm']\n",
        "        print(f\"\\n{algo.upper()}:\")\n",
        "        print(f\"  File count: {result['file_count']:,}\")\n",
        "        print(f\"  Total size: {result['total_size_gb']:.3f} GB\")\n",
        "        print(f\"  Validation time: {result['validation_time_sec']:.2f} seconds ({result['validation_time_min']:.2f} minutes)\")\n",
        "        print(f\"  Objects per second: {result.get('objects_per_second', 0):.2f}\")\n",
        "        \n",
        "        # Size and time overhead\n",
        "        if baseline is not None:\n",
        "            size_oh = ((result['total_size_gb'] / baseline['total_size_gb'] - 1) * 100)\n",
        "            time_oh = ((result['validation_time_sec'] / baseline['validation_time_sec'] - 1) * 100)\n",
        "            print(f\"  Size overhead: {size_oh:+.1f}%\")\n",
        "            print(f\"  Time overhead: {time_oh:+.1f}%\")\n",
        "        \n",
        "        # Signature verification details (comprehensive)\n",
        "        sig_ver = result.get('signature_verification', {})\n",
        "        if sig_ver and isinstance(sig_ver, dict) and sig_ver.get('sampled', 0) > 0:\n",
        "            print(f\"  Signature verification:\")\n",
        "            print(f\"    Sampled: {sig_ver.get('sampled', 0):,}\")\n",
        "            print(f\"    Verified: {sig_ver.get('verified', 0):,} ({sig_ver.get('verification_rate_pct', 0):.1f}%)\")\n",
        "            print(f\"    Failed: {sig_ver.get('failed', 0):,}\")\n",
        "            print(f\"    ASN.1 errors: {sig_ver.get('asn1_extraction_failures', 0):,}\")\n",
        "            print(f\"    Avg verify time: {sig_ver.get('avg_verify_time_ms', 0):.2f} ms\")\n",
        "            print(f\"    Verification rate: {sig_ver.get('verification_rate_per_sec', 0):.1f} sig/s\")\n",
        "            print(f\"    Time percentiles: P25={sig_ver.get('p25_verify_time_ms', 0):.2f}, P50={sig_ver.get('p50_verify_time_ms', 0):.2f}, P75={sig_ver.get('p75_verify_time_ms', 0):.2f}, P95={sig_ver.get('p95_verify_time_ms', 0):.2f}, P99={sig_ver.get('p99_verify_time_ms', 0):.2f} ms\")\n",
        "            print(f\"    Signature size: {sig_ver.get('signature_size_avg_bytes', 0):.0f} bytes (expected: {sig_ver.get('expected_signature_size_bytes', 0)})\")\n",
        "            print(f\"    Public key size: {sig_ver.get('public_key_size_avg_bytes', 0):.0f} bytes (expected: {sig_ver.get('expected_public_key_size_bytes', 0)})\")\n",
        "            \n",
        "            # Per-type metrics summary\n",
        "            ptm = sig_ver.get('per_type_metrics', {})\n",
        "            if isinstance(ptm, dict):\n",
        "                print(f\"    Per-type breakdown:\")\n",
        "                for obj_type, metrics in ptm.items():\n",
        "                    if isinstance(metrics, dict) and metrics.get('count', 0) > 0:\n",
        "                        print(f\"      {obj_type}: {metrics.get('count', 0)} objects, {metrics.get('verified', 0)} verified, {metrics.get('failed', 0)} failed\")\n",
        "                        if metrics.get('ee_certs_found', 0) > 0:\n",
        "                            print(f\"        EE certs: {metrics.get('ee_certs_found', 0)}, Issuer certs: {metrics.get('issuer_certs_found', 0)}\")\n",
        "                            print(f\"        CMS valid: {metrics.get('cms_valid_count', 0)}, EE valid: {metrics.get('ee_cert_valid_count', 0)}, Both: {metrics.get('both_valid_count', 0)}\")\n",
        "        \n",
        "        # File type breakdown\n",
        "        ftb = result.get('file_type_breakdown', {})\n",
        "        if isinstance(ftb, dict) and any(ftb.values()):\n",
        "            print(f\"  File types: {', '.join([f'{k}:{v:,}' for k, v in ftb.items() if v > 0])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
